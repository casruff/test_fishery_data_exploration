---
title: Skagit Steelhead Test Fishery Data Exploration
author: Thomas Buehrens (Washington Department of Fish and Wildlife, tbuehrens@dfw.wa.gov), Bob McClure (Upper Skagit Indian Tribe, bobm@upperskagit.com), Garrett Rowles (Upper Skagit Indian Tribe, growles@upperskagit.com), and Casey Ruff (Swinomish Tribe, cruff@swinomish.nsn.us)
output:
  html_document:
    code_folding: hide
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

***

Last Updated `r format(Sys.time(), '%m/%d/%Y')`.

***


# Overview
This script fits a regression model to Skagit Steelhead Test Fishery Data with Total Run Size as an offset. In years prior to the final year, the "observed" TRS is supplied as the offset. In the final In-Season-Update (ISU) year, the mean and sd of the preseason forecast are supplied as priors for the TRS. When this model is fit with no data for the final year, it returns exactly* the preseason forecast. As ISU test fishery data is added, the model progressively updates the pre-season forecast, providing an ISU estimate of runsize.

*the model will not produce EXACTLY the preseason forecast with no ISU data. It is using a log-normal distribution to approximate the probability density function of the pre-season forecast, which is very good but inexact so actual forecasts with no ISU data may vary by 1-2% of the "true" preseason forecast.

The code repository used to generate this page and complete analyses can be found here: [**(link)**](https://github.com/casruff/test_fishery_data_exploration)

## Setup
All analyses require R software [**(link)**](https://cran.r-project.org/) (v3.4.3) for data retrieval, data processing, and summarizing model results. Here we configure R to perform our analysis and generate our outputs
```{r set_options, echo = TRUE, message = FALSE}
options(width = 100)
knitr::opts_chunk$set(message = FALSE)
set.seed(123)
```

We also need a couple of helper functions which we will define
```{r load_funcs, message = FALSE, warning = FALSE,results = "hide"}
#function to install or load packages
install_or_load_pack <- function(pack){
  create.pkg <- pack[!(pack %in% installed.packages()[, "Package"])]
  if (length(create.pkg))
    install.packages(create.pkg, dependencies = TRUE)
  sapply(pack, require, character.only = TRUE)
}

```

Here we will load & install packages we need to use (needs internet connection if packages not already installed)
```{r load_packages, message = FALSE, warning = FALSE,results = "hide"}
packages_list<-c("tidyverse"
                 ,"forecast"
                 ,"mgcv"
                 ,"ggplot2"
                 ,"MASS"
                 ,"RColorBrewer"
                 ,"kableExtra"
                 ,"gtools"
                 ,"readr"
                 ,"here"
                 ,"readxl"
                 ,"brms"
                 ,"bsplus"
                 ,"lubridate"
                 ,"MuMIn"
                 ,"tidymv"
                 ,"modelr"
                 ,"R2jags"
                 ,"kableExtra"
                 # ,"rnoaa"
                 # ,"ncdf4"
                 # ,"ncdf4.helpers"
                 # ,"raster"
                 ,"reshape2"
                 # ,"ggfortify"
                 )
install_or_load_pack(pack = packages_list)
```

## User Inputs
Here we will specify the years of analysis, identify data file names and the flow gauge to be used.
```{r user_inputs, message = FALSE, warning = FALSE,results = "show"}
## set data dir
datadir <- here("data")
modeldir <- here("jags")
savedir <- here("results")

## first & last years of fish data
yr_frst <- 2013
yr_last <- 2023

##years for generating 1 step ahead forecasts
years <- c(2017,2019,2020,2022,2023,2024)

n_for <- length(years)

## data file names
## 1. file with escapement data
fn_trs <- "sthd_trs.csv"

## 2. file with age comp data
fn_tst_fsh <- "skgt_sthd_tst_fsh_update.csv"

## get flow data (currently using mainstem at mount vernon)
## flow gage ID
flow_site <- 12200500  
```

```{r define_funcs}
fit_load_mods <- function(n_for,years){
  
  ## empty list for fits
  mod_fits <- vector("list", n_for)
  
  for(n in 1:n_for){
    if(file.exists(file.path(savedir,paste(model,"_","y",i,".rds",sep = "")))) {
        mod_fits[[t]] <- readRDS(file.path(savedir,paste(model,"_","y",i,".rds",sep = "")))
    } else{}
  }
  
}

```


## Load Data
In this section we will load 1) the Total Run Size data, 2) the Test Fishery Data, and 3) The flow data.
```{r load_data, message = FALSE, warning = FALSE,results = "show"}
## read total runsize data
dat_trs <- read_csv(file.path(datadir,fn_trs))
## read in test fishery data
dat_tst_fsh <- read_csv(file.path(datadir,fn_tst_fsh))%>%
  mutate(date = mdy(date),week = week(ymd(date)))



## get URL for flow data from USGS
flow_url <- paste0("https://waterdata.usgs.gov/nwis/dv",
                   "?cb_00060=on",
                   "&format=rdb",
                   "&site_no=",flow_site,
                   "&begin_date=",yr_frst,"-01-01",
                   "&end_date=",yr_last,"-12-31")

## raw flow data from USGS
skip <- read_lines(flow_url)%>%
  as_tibble()%>%
  filter(grepl("\\#",value))%>%
  summarise(skip=n())%>%
  unlist()

## flow data for years of interest
dat_flow <-  read_tsv(flow_url,
                      col_names = FALSE,
                      col_types = "ciDdc",
                      skip = skip + 2 )%>%
  dplyr::rename(date=X3,flow=X4)%>%
  dplyr::select(date,flow)%>%
  mutate(flow = flow / 35.3147,
         year= year(date),
         day = yday(date),
         month = month(date)
  )

#current week of data for 1 step ahead forecast
currentWeek <-  dat[dim(dat)[1],"week"]

```
## ARIMA model with preseason forecast 
This model estimates the Total Run Size (TRS) by predicting test fishery catch using regression model with the following equation:

catch ~ negative binomial (prediction, over-dispersion parameter) 

where

prediction = exp(intercept + random effect(site) + factor(set number) + random walk (day of year) + log(effort in seconds) + log(centered year effect))

The log(centered year effect) is given as data for all but the final incomplete year where TRS is unknown. It is calculated as the log(TRS) - log(mean(TRS[1:final_year-1])). The log(centered year effect) for the final incomplete year is given a normally distributed prior by subtracting log(mean(TRS[1:final_year-1])) from the log of a set of mcmc draws from the preseason forecast probability density function and calculating the mean and the sd of the resulting set of mcmc draws. Thus the model produces exactly* the preseason forecast when no new ISU data is provided for the final year, and then as ISU data is added, progressively updates this pre-season forecast providing ISU estimates of TRS.

*the model will not produce EXACTLY the preseason forecast with no ISU data. It is using a log-normal distribution to approximate the probability density function of the pre-season forecast, which is very good but inexact so actual forecasts with no ISU data may vary by 1-2% of the "true" preseason forecast.
```{r Analysis_pre_season, message=FALSE, warning=FALSE, results="show"}
dat<-dat_tst_fsh%>%
  filter(year <= yr_last)%>%
  left_join(dat_trs)%>%
  left_join(dat_flow)%>%
  mutate(site=factor(site),
         zl_flow=scale(log(flow)),
         effort = as.numeric(time_out-time_in)*60*60,
         lm_trs = log(mean(unique(trs),na.rm = TRUE))
         )%>%
  filter(set_num < 3)%>%
  #filter(week < 16)%>%
  add_rownames()%>%
  mutate(year = year - min(year) + 1,
         day = day - min(day,na.rm = TRUE) + 1,
         period = (year-1) * max(day,na.rm = TRUE) + day
         )%>%
  filter(!is.na(day))


write.csv(dat,"dat.csv")

# forecast<-read_csv("https://raw.githubusercontent.com/casruff/Skagit-River-Steelhead-Forecast/master/analysis/cache/ensemble_forecast_posterior.csv")%>%
#   mutate(log_F=log(ensemble_forecast_posterior),
#          log_F_anom = log_F - unique(dat$lm_trs)
#          )

forecast<-read_csv(file.path(datadir,"ensemble_forecast_posterior_retrospective.csv"))%>%
  mutate(log_F=log(ensemble_forecast_posterior_2024),
         log_F_anom = log_F - unique(dat$lm_trs)
         )

jagsdat<-list(
  S = max(dat$day,na.rm = TRUE) - min(dat$day, na.rm = TRUE) + 1,
  Y = max(dat$year) - min(dat$year) + 1,
  sites = length(unique(dat$site)),
  netsets = length(unique(dat$set_num)),
  site = as.numeric(dat$site),
  set_num = as.numeric(dat$set_num),
  effort = dat$effort/(60*60),
  lm_trs = dat$lm_trs,
  ##====================================================
  # run with these turned on to show in-season forecast
  #=====================================================
  obs = dat%>%nrow(),
  SthdW = dat%>%dplyr::select(SthdW)%>%unlist(),
  year = dat%>%dplyr::select(year)%>%unlist(),
  day = dat%>%dplyr::select(day)%>%unlist(),
  period = dat%>%dplyr::select(period)%>%unlist(),
  ##====================================================
  # run with these turned on to show pre-season forecast
  #=====================================================
  # obs = dat%>%filter(year<max(year))%>%nrow(),
  # SthdW = dat%>%filter(year<max(year))%>%dplyr::select(SthdW)%>%unlist(),
  # year = dat%>%filter(year<max(year))%>%dplyr::select(year)%>%unlist(),
  # day = dat%>%filter(year<max(year))%>%dplyr::select(day)%>%unlist(),
  # period = dat%>%filter(year<max(year))%>%dplyr::select(period)%>%unlist(),
  #======================================================
  Y_obs = length(unique(dat$year)),
  ylist = unique(dat$year),
  log_trs_anom = dat%>%group_by(year)%>%summarise(trs=first(log(trs)))%>%dplyr::select(trs)%>%unlist()-unique(dat$lm_trs),
  mu_log_forcast_anom = mean(forecast$log_F_anom),
  sd_log_forcast_anom = sd(forecast$log_F_anom)
)

model<-"model
{
  #Data
  # S number of periods in season
  # Y number of years
  # sites number of sites
  # netsets max number of net sets
  # site site for each obs
  # set_num set number for each obs
  # effort effort in hrs
  # lm_trs log(mean(total run size)) 
  # obs  observation number
  # SthdW wild steelhead catch
  # year year index for obs
  # ylist list of unique year indexes
  # mu_log_forcast_anom log of preaseason forecast median minus lm_trs
  # sd_log_forcast_anom sd(log(preseason forecast pdf minus lm_trs))
  
  #Process Model
  for(i in 2:S){
    eps[i-1] ~ dnorm(0,1^-1)
    resid[i] = resid[i-1] + eps[i-1] * sigma
  }
  #random effects{
  for(i in 2:sites){
    b1[i] = eps_site[i-1] * sigma_site
  }
  
  #Likelihood
  for(i in 1:obs){
    log_lambda[i] = intercept + b1[site[i]] + b2[set_num[i]] + log(effort[i]) + b3[year[i]] + resid[day[i]]
    p[i] <- r / (r + exp(log_lambda[i]))
    SthdW[i] ~ dnegbin(p[i], r)
  }
  
  #Priors
  sigma ~ dt(0,2.5^-2,1) T(0,)
  sigma_site ~ dt(0,2.5^-2,1) T(0,)
  sigma_disp ~ dt(0,2.5^-2,1) T(0,)
  r = sigma_disp^-2 #negative binomial overdispersion parameter
  intercept ~ dnorm(0,10^-2)
  b1[1] = 0;
  b2[1] = 0;
  resid[1] = 0;
  for(i in 2:sites){
    eps_site[i-1] ~ dnorm(0,1^-1)
  }  
  for(i in 2:netsets){
    b2[i] ~ dnorm(0,5^-2) 
  }
  for(i in 1:(Y_obs-1)){
    b3[ylist[i]] = log_trs_anom[i]
  }
  b3[ylist[Y_obs]] ~ dnorm(mu_log_forcast_anom,sd_log_forcast_anom^-2)
}"

write(model,"jags/model_pre_season.jags")  

  
params=c(
  "sigma",
  "sigma_disp",
  "sigma_site",
  "r",
  "p",
  "intercept",
  "b1",
  "b2",
  "b3",
  "resid",
  "eps",
  "eps_site",
  "log_lambda"
)

if(!file.exists("results/fit_pre_season.RDS")){
  start<-Sys.time()
  print(start)
  fit<-jags.parallel(
    data=jagsdat,
    inits=NULL,
    parameters.to.save = params,
    model.file = "jags/model_pre_season.jags",
    n.chains = 4,
    n.iter = 2000,
    n.burnin = 1000,
    n.thin = 1
  )
  stop<-Sys.time()
  print(stop-start)
  write.csv(fit$BUGSoutput$summary,"results/summary_pre_season.csv")
  saveRDS(fit,"results/fit_pre_season.RDS")
}else{
  fit<-readRDS("results/fit_pre_season.RDS")
}


abundance<-tibble(melt(exp(fit$BUGSoutput$sims.list$b3 + unique(dat$lm_trs))))%>%
  rename(draw=Var1,year=Var2,abundance=value)%>%
  group_by(year)%>%
  summarise(abundance = quantile(abundance, c(0.025, 0.25, 0.5, 0.75,0.975)), q = c(0.025, 0.25, 0.5, 0.75,0.975))%>%
  pivot_wider(id_cols=year,names_from = q,values_from = abundance)%>%
  bind_cols(dat%>%dplyr::select(year_real=year,trs)%>%group_by(year_real)%>%summarise(trs=first(trs)))%>%
  dplyr::select(!year)%>%
  mutate(year= year_real + min(dat_tst_fsh$year)-1)

ggplot(abundance,aes(x=year,y=trs))+
  geom_ribbon(mapping = aes(x=year,ymin =`0.025`, ymax = `0.975`),alpha=0.25,fill="red")+
  geom_ribbon(mapping = aes(x=year,ymin =`0.25`, ymax = `0.75`),alpha=0.25,fill="red")+
  geom_line(mapping=aes(x=year,y=`0.5`),color="red")+
  geom_point()+
  ylim(0,NA)

abundance%>%
  dplyr::select(year,trs,`0.025`,`0.25`, `0.5`,`0.75`,`0.975`)%>%
  ungroup()%>%
  dplyr::select(!trs)%>%
  filter(year == max(year))%>%
  kbl(caption = "Table 1. In-Season-Update of Predicted TRS",digits =0)%>%
  kable_classic(full_width = F, html_font = "Cambria")

day_effect<-tibble(melt(exp(fit$BUGSoutput$sims.list$resid)))%>%
  rename(draw=Var1,day=Var2,day_effect=value)%>%
  group_by(day)%>%
  mutate(day=day+min(yday(dat$date))-1)%>%
  summarise(day_effect = quantile(day_effect, c(0.025, 0.25, 0.5, 0.75,0.975)), q = c(0.025, 0.25, 0.5, 0.75,0.975))%>%
  pivot_wider(id_cols=day,names_from = q,values_from = day_effect)

ggplot(day_effect,aes(x=day,y=`0.5`))+
  ylab("Day of Year Effect (relative to first day of test fishery")+
  xlab("Day of Year")+
  geom_ribbon(mapping = aes(x=day,ymin =`0.025`, ymax = `0.975`),alpha=0.25,fill="red")+
  geom_ribbon(mapping = aes(x=day,ymin =`0.25`, ymax = `0.75`),alpha=0.25,fill="red")+
  geom_line(mapping=aes(x=day,y=`0.5`),color="red")+
  geom_point()+
  ylim(0,NA)
```
